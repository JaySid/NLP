
Summarization Model Fine-Tuning Scripts
This repository contains scripts for fine-tuning LED, Long T5, and Pegasus models for summarization tasks. These files are designed to help researchers and developers efficiently adapt these models for domain-specific or custom summarization tasks.

Contents
LED Fine-Tuning Script: Code for fine-tuning the Longformer Encoder-Decoder (LED) model, optimized for long-document summarization.
Long T5 Fine-Tuning Script: Script to fine-tune Long T5, a variant of T5 tailored for long input sequences.
Pegasus Fine-Tuning Script: Code for fine-tuning Google's Pegasus model, designed for abstractive summarization tasks.
Requirements
Before running the scripts, ensure the following are installed:

Python 3.8 or later
PyTorch 1.10 or later
Hugging Face Transformers library
CUDA (for GPU acceleration)


Summarization Model Fine-Tuning Scripts
This repository contains scripts for fine-tuning LED, Long T5, and Pegasus models for summarization tasks. These files are designed to help researchers and developers efficiently adapt these models for domain-specific or custom summarization tasks.

Contents
LED Fine-Tuning Script: Code for fine-tuning the Longformer Encoder-Decoder (LED) model, optimized for long-document summarization.
Long T5 Fine-Tuning Script: Script to fine-tune Long T5, a variant of T5 tailored for long input sequences.
Pegasus Fine-Tuning Script: Code for fine-tuning Google's Pegasus model, designed for abstractive summarization tasks.
Requirements
Before running the scripts, ensure the following are installed:

Python 3.8 or later
PyTorch 1.10 or later
Hugging Face Transformers library
CUDA (for GPU acceleration)


